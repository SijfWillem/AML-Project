{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'classifier.weight', 'roberta.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForMultipleChoice.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_data_for_roberta(json_file_path):\n",
    "    # Load the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Format data\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "\n",
    "        # Loop through each answer\n",
    "        for i in range(4):\n",
    "            answer_key = f'answer{i}'\n",
    "            answer = item[answer_key]\n",
    "\n",
    "            formatted_data.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'correct_answer': correct_answer == answer\n",
    "            })\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Example usage\n",
    "json_file_path = 'dataset.json'  # Replace with your file path\n",
    "formatted_data = format_data_for_roberta(json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name for the JSON file\n",
    "file_name = 'roBERTa.json'\n",
    "\n",
    "# Writing the list of JSON objects to a file\n",
    "with open(file_name, 'w') as file:\n",
    "    json.dump(formatted_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9932dc55c7a45d9b3d31098ec395000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afd8c2e9a4d457f93fb5d66802bb581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c0893f238b47908ca6d4a1c9c41b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "ds = DatasetDict.from_json({'train': 'roBERTa.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = ds[\"train\"].train_test_split(test_size=0.3)\n",
    "\n",
    "# Combine the new train and validation set with the other sets in the original DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In the argument, the portion in boldface plays which of the following roles?',\n",
       " 'context': \"Country X' s recent stock-trading scandal should not diminish investors' confidence in the country's stock market. For one thing, <b> the discovery of the scandal confirms that Country X has a strong regulatory system </b>, as the following considerations show. In any stock market, some fraudulent activity is inevitable. If a stock market is well regulated, any significant stock-trading fraud in it will very likely be discovered. This deters potential perpetrators and facilitates improvement in regulatory processes.\",\n",
       " 'answer': \"It is a conclusion for which the argument provides support and which itself is used to support the argument's main conclusion.\",\n",
       " 'correct_answer': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# Function to tokenize data\n",
    "def tokenize_roberta_data(formatted_data):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for item in formatted_data:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            item['context'] + \" \" + tokenizer.sep_token + \" \" + item['question'] + \" \" + tokenizer.sep_token + \" \" + item['answer'],\n",
    "            add_special_tokens = True,\n",
    "            padding = 'max_length',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt',\n",
    "        )\n",
    "        tokenized_data.append({\n",
    "            'input_ids': encoded_dict['input_ids'],\n",
    "            'attention_mask': encoded_dict['attention_mask'],\n",
    "            'label': 1 if item['correct_answer'] else 0\n",
    "        })\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_data_train = tokenize_roberta_data(ds['train'])\n",
    "tokenized_data_val = tokenize_roberta_data(ds['validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Country X' s recent stock-trading scandal should not diminish investors' confidence in the country's stock market. For one thing, <b> the discovery of the scandal confirms that Country X has a strong regulatory system </b>, as the following considerations show. In any stock market, some fraudulent activity is inevitable. If a stock market is well regulated, any significant stock-trading fraud in it will very likely be discovered. This deters potential perpetrators and facilitates improvement in regulatory processes. </s> In the argument, the portion in boldface plays which of the following roles? </s> It is a conclusion for which the argument provides support and which itself is used to support the argument's main conclusion.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "def show_original_sample(tokenized_data, tokenizer, sample_index):\n",
    "    # Extract the token IDs for the sample\n",
    "    input_ids = tokenized_data[sample_index]['input_ids'].squeeze()\n",
    "\n",
    "    # Decode the token IDs back to text\n",
    "    decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "\n",
    "    return decoded_text\n",
    "\n",
    "# Example usage\n",
    "sample_index = 0  # Index of the sample you want to display\n",
    "original_text = show_original_sample(tokenized_data_train, tokenizer, sample_index)\n",
    "\n",
    "print(original_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12986"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([d['label'] for d in tokenized_data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMultipleChoice, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Assuming tokenized_train_data and tokenized_val_data are already prepared\n",
    "# Each should be a list of dictionaries with 'input_ids', 'attention_mask', and 'label'\n",
    "\n",
    "def create_dataloader(tokenized_data, batch_size):\n",
    "    input_ids = torch.stack([d['input_ids'] for d in tokenized_data])\n",
    "    attention_masks = torch.stack([d['attention_mask'] for d in tokenized_data])\n",
    "    labels = torch.tensor([d['label'] for d in tokenized_data])\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = create_dataloader(tokenized_data_train, batch_size=8)\n",
    "val_dataloader = create_dataloader(tokenized_data_val, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'classifier.weight', 'roberta.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sijf/Desktop/UvA/Master /Blok 2/AML/reading-comprehension-with-logic/AML-RoBERTa/roBERTa.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sijf/Desktop/UvA/Master%20/Blok%202/AML/reading-comprehension-with-logic/AML-RoBERTa/roBERTa.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m b_input_ids, b_attention_masks, b_labels \u001b[39m=\u001b[39m batch    \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sijf/Desktop/UvA/Master%20/Blok%202/AML/reading-comprehension-with-logic/AML-RoBERTa/roBERTa.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sijf/Desktop/UvA/Master%20/Blok%202/AML/reading-comprehension-with-logic/AML-RoBERTa/roBERTa.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49mb_input_ids, attention_mask\u001b[39m=\u001b[39;49mb_attention_masks, labels\u001b[39m=\u001b[39;49mb_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sijf/Desktop/UvA/Master%20/Blok%202/AML/reading-comprehension-with-logic/AML-RoBERTa/roBERTa.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sijf/Desktop/UvA/Master%20/Blok%202/AML/reading-comprehension-with-logic/AML-RoBERTa/roBERTa.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1327\u001b[0m, in \u001b[0;36mRobertaForMultipleChoice.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(reshaped_logits\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1326\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1327\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(reshaped_logits, labels)\n\u001b[1;32m   1329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1330\u001b[0m     output \u001b[39m=\u001b[39m (reshaped_logits,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 1 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = RobertaForMultipleChoice.from_pretrained('roberta-base')\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, no_deprecation_warning=True)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 3  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids, b_attention_masks, b_labels = batch    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=b_input_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
    "        print(outputs)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Training loss: {avg_train_loss}\")\n",
    "\n",
    "    # # Validation Loop\n",
    "    # model.eval()\n",
    "    # total_eval_accuracy = 0\n",
    "\n",
    "    # for batch in val_dataloader:\n",
    "    #     b_input_ids, b_attention_masks, b_labels = batch\n",
    "        \n",
    "    #     with torch.no_grad():\n",
    "    #         outputs = model(input_ids=b_input_ids, attention_mask=b_attention_masks)\n",
    "        \n",
    "    #     logits = outputs.logits\n",
    "    #     predictions = torch.argmax(logits, dim=-1)\n",
    "    #     total_eval_accuracy += torch.sum(predictions == b_labels)\n",
    "\n",
    "    # avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    # print(f\"Epoch {epoch+1} | Validation Accuracy: {avg_val_accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('roberta-bertje')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
